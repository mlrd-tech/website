[{"id":0,"href":"/docs/v0-beta/quick-start/","title":"Quick Start","section":"v0-beta","content":"Quick Start# Example table:\nName: Books Primary Key: Partition Key: Title (string) Sort Key: Year (number) Indexes: ISBN Partition Key: ISBN (string) New Table# Write table config Books.yaml: ddb: table_name: Books primary_key: partition_key: \u0026#34;S,Title\u0026#34; sort_key: \u0026#34;N,Year\u0026#34; indexes: ISBN: partition_key: \u0026#34;S,ISBN\u0026#34; mysql: hostname: 10.0.0.75:3306 username: mlrd password: ${MLRD_MYSQL_PASSWORD_BOOKS} # env var db: mlrd Start mlrd $ mlrd Books.yaml Reconfigure App Varies by langauge and AWS SDK, but goal is to set:\nRegion: \u0026ldquo;local\u0026rdquo; is a good value since mlrd is local to you Credentials provider: AWS access keys and IAM auth are not needed with mlrd BaseEndpoint: URL of your mlrd server In Go with AWS SDK v2:\nimport ( \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/credentials\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/dynamodb\u0026#34; ) opts := dynamodb.Options{ Region: \u0026#34;local\u0026#34;, Credentials: credentials.NewStaticCredentialsProvider(\u0026#34;local\u0026#34;, \u0026#34;local\u0026#34;, \u0026#34;local\u0026#34;), BaseEndpoint: aws.String(\u0026#34;http://10.0.0.1:7999\u0026#34;), DisableValidateResponseChecksum: true, } client := dynamodb.New(opts) AWS Config\nThere are many ways to configure AWS. Consult the AWS SDK docs for your language and SDK.\nTest App with Existing Table# Follow the steps above but remove the mysql section from Book.yaml\nA table config with no mysql section or migration_step setting puts the table in transparent mode, which makes mlrd act like a transparent proxy for the table and only return DynamoDB errors; mlrd errors are only logged.\nRun mlrd for days or weeks to ensure that, at a basic level, it can handle the application-specific workload.\n"},{"id":1,"href":"/docs/v0-beta/high-level/","title":"High Level","section":"v0-beta","content":"High Level mlrd Server# mlrd is a DynamoDB-compatible API that uses an SQL store (currently only MySQL) Clients (i.e. the application) connect to mlrd instead of DynamoDB The default mlrd port is 7999 A single mlrd instance can handle many tables Multiple mlrd instances can be run for the same tables, but currently mlrd uses a shared-nothing design; therefore, be careful to configure each mlrd instances identically (a future version of mlrd will use a shared control plane) The mlrd server (API, logging, etc.) is configured with an optional YAML file called a \u0026ldquo;sys config\u0026rdquo; Use the -sys command line flag to set a sys config mlrd defaults to structured log output on STDOUT with log levels error, warn, info, and debug for CreateTable requests mlrd prints to STDERR only if startup fails before initializing logging; else, mlrd logs only to STDOUT as configured Table configs are specified as command line arguments (after any command line flags), like mlrd tbl_A.yaml tbl_B.yaml If no table configs are specified, mlrd uses it controlPlane sys config settings to create new tables for CreateTable requests On startup, mlrd validates config files, initialize logging and metrics, creates DynamoDB clients, checks each table config and corresponding SQL table, adds missing SQL tables or indexes, and starts the API Use the -initcheck command line flag to run the startup sequence then exit; if successful, it prints \u0026ldquo;init check OK\u0026rdquo; and exits zero; else it prints an error and exists non-zero Currently, mlrd does not use or set a PID file To stop mlrd, send an interrupt or SIGTERM signal Tables# Tables are uniquely identified by a case-sensitive name Each table must be defined in YAML file called a \u0026ldquo;table config\u0026rdquo; Each table config is completely self-contained and independent; if, for example, two tables use the same MySQL settings, both table configs need a copy of those MySQL settings The minimal table config defines ddb.tableName and ddb.primaryKey; this would define a table in transparent mode Each table runs in one table mode: transparent: mlrd acts likes a transparent proxy for all requests; used to test and verify mlrd with the application migration: mlrd dual-writes and dual-reads DynamoDB and the SQL store depending on the migration step; a migration moves data from DynamoDB to the SQL store, ultimately allowing the application to stop using the DynamoDB table sql: mlrd uses only the SQL store; DynamoDB is not accessed A transparent mode table returns only DynamoDB errors; mlrd errors, if any, are only logged Before migrating an existing DynamoDB table to the SQL store, the table should be run in transparent mode for days or weeks to ensure that mlrd can parse and handle all request parameters Migrating a DynamoDB table to the SQL store is a four-part, multi-step process that should be performed by experienced app developers and DBAs Net-new tables can be created in SQL mode by specifying a table config that includes a ddb section, a mysql section, and no migration value. For all table modes, a table config is required that defines the table in the ddb section Metrics# mlrd collects and reports a comprehensive set of counter and gauge metrics Metrics are reported per-table with a table_name tag (aka label, attribute, or dimension) Metrics are OpenTelemetry and reported to an OpenTelemetry endpoint Metric reporting frequency is as fast as 1 second (maximum resolution); the default is 5 seconds Supported Requests# mlrd is fully-compatible with the DynamoDB data plane classic API: GetItem, BatchGetItem, Query, Scan, PutItem, UpdateItem, DeleteItem, BatchWriteItem Support for TransactGetItems and TransactWriteItems are still in development mlrd is fully-compatible with all expressions: ConditionExpression, FilterExpression, UpdateExpression, KeyConditionExpression, and ProjectionExpression. "},{"id":2,"href":"/docs/v0-beta/concepts/","title":"Concepts","section":"v0-beta","content":"mlrd Concepts Codes# What DynamoDB calls \u0026ldquo;Data type descriptors\u0026rdquo;, mlrd calls \u0026ldquo;codes\u0026rdquo;:\nCode Value Type S string N number B binary BOOL boolean NULL null M map L list SS string set NS number set BS binary set Coded Attribute# String with format \u0026ldquo;\u0026lt;code\u0026gt;,\u0026lt;attributeName\u0026gt;\u0026rdquo;.\nExample: \u0026ldquo;S,Titlte\u0026rdquo;: code S (string value); attribute name \u0026ldquo;Title\u0026rdquo;.\nCoded attributes are used to define indexes.\nEmpty Table# Every SQL database requires a table called _empty with a single row:\nmysql\u0026gt; SELECT * FROM _empty; +----+------+ | pk | item | +----+------+ | 0 | {} | +----+------+mlrd checks for and creates this table and row on startup.\nThe empty table is needed to make certain condition expressions work in SQL.\nFault# A fault is a unrecoverable error that means data integrity has mostly likely be compromised. There are the possible faults:\nSQL trx commit fails SQL trx rollback fails Update return projection error Faults increment the fault counter metric.\nTable Name# A table can (but usually does not) have three names referred to by these terms:\nTerm Refers To Config client table name Table name in client requests DynamoDB table name Table name in DynamoDB ddb.tableName SQL table name Table name in SQL database mysql.tableName \u0026ldquo;Table name\u0026rdquo; always refers to the client table name. And since all three are equal by default, \u0026ldquo;table name\u0026rdquo; usually refers to any of the three. When ambiguous, a specific term is used.\nLogging and metrics always use the client table name.\nAll table names are case-sensitive.\nTop-level Attribute# A top-level attribute is not nested (accessed through) another attribute.\n{ \u0026#34;Title\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;OK Computer\u0026#34;}, \u0026#34;Meta\u0026#34;: { \u0026#34;M\u0026#34;: { \u0026#34;Label\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Capital\u0026#34;} } } }Title is a top-level attribute. Meta.Label is not.\nTop-level attributes with codes S, N, and B can be used in indexes.\n"},{"id":3,"href":"/docs/v0-beta/","title":"v0-beta","section":"mlrd Docs","content":"mlrd v0-beta Docs Beta Preview Docs\nmlrd is in private beta, so these docs are only a preview.\nInformation in these docs will change.\n"},{"id":4,"href":"/docs/table-modes/","title":"Table Modes","section":"mlrd Docs","content":"Table Modes# A table runs in a mode determined by its table config:\nTable Mode Table Config DynamoDB Access SQL Store Access Source of Truth SQL ddb\nmysql ✅ SQL store Migration ddb\nmigration\nmysql ✅ ✅ Depends on migration step Transparent ddb ✅ DymamoDB SQL# SQL table mode uses only a SQL store; there is no DynamoDB access.\nUse this mode for net-new tables.\nMigration# Migration table mode is used to move a table from DynamoDB to a SQL store.\nSetting migration_step in a table config puts the table in migration mode.\nThe difference between transparent mode and the ddb migration step is that mlrd returns parsing errors to the client in migration mode.\nMigrating a table is a multi-step process that requires careful planning and execution. Read the migration docs carefully and have another engineer double-check everything.\nMigration mode is the path to the end goal of mlrd: SQL table mode.\nTransparent# Transparent table mode is used to test mlrd with a table in DynamoDB (not a SQL store).\nIn transparent mode, mlrd acts as a transparent proxy between the client and DynamoDB. All requests are sent to DynamoDB. All values and errors (if any) from DynamoDB are sent back to the client.\nBefore sending a request to DynamoDB, mlrd parses the request. mlrd parsing errors are only logged, not returned to the client.\nBasic Request Testing\nTransparent mode tests only request parsing (expressions, parameters, and so on). A table that works in transparent mode might fail in migration or SQL mode.\nTransparent mode is a precursor to the first migration step: ddb.\n"},{"id":5,"href":"/docs/lockout/","title":"Lockout","section":"mlrd Docs","content":"Lockout To safeguard data integrity, mlrd enables read-only and lockout on a table due to a fault or write barrier. Lockout means read-only cannot be turned off dynamically.\nWARNING\nDo not restart mlrd without analyzing and resolving the error that cuased the lockout! Failure to do so could cause data corruption in DynamoDB and the SQL store.\nFirst check the logs to see if the lockout was due to a fault or write barrier. Both are clearly logged with the word \u0026ldquo;FAULT\u0026rdquo; or \u0026ldquo;write barrier\u0026rdquo;.\nFault# Write Barrier# "},{"id":6,"href":"/docs/v0-beta/config/command-line/","title":"Command Line","section":"Config","content":"Command Line Flags and Args $ ./mlrd --help Usage: mlrd [flags] [table_config_file...] Flags: -debug debug log level -initcheck initialize and exit -sys string sys config file -version print version and exitFlags# Flags must be specified first (before arguments). Short (-version) and long (--version) syntaxes work. All flags are optional.\nArgs# Arguments must be specified after flags. Every argument is interpreted as a table config file.\nSignals# mlrd does a clean shutdown on interrupt and term signals.\nExit Status# Standard Unix exit status: 0 = no errors; non-zero = error (see log)\n"},{"id":7,"href":"/docs/v0-beta/config/feature-flags/","title":"Feature Flags","section":"Config","content":"Feature Flags Flag Disables sql-create-data-table Automatic data table creation on startup sql-create-empty-table Automatic empty table creation on startup sql-init-empty-table Inserting empty item into empty table on startup sql-data-table-check Checking data table (columns, indexes, etc.) on startup sql-empty-table-check Checking empty table on startup sql-index-check Checking indexes in data tables on startup sql-create-indexes Creating missing indexes in data tables on startup sql-index-visible-warning Warning about SQL data table indexes that are visible but not defined in the table config sql-index-invisible-warning Warning about SQL data table indexes that are invisible aws-auto-region Auto-detecting AWS region using IMDS aws-rds-tls Automatically using AWS RDS CA strict-index-projection DynamoDB strict projection expression for secondary indexes ddb-auto-table Using DescribeTable to automatically define primary key and secondary table indexes in the table config write-new-config-files API control plane writing new table config files for newly created tables mysql-tls Using TLS for MySQL TCP connections safe-logging DO NOT DISABLE IN PRODUCTION\nRedaction of data values in log messages, including API requests and SQL queries—this is very dangerous! "},{"id":8,"href":"/docs/v0-beta/config/standard/","title":"Standard","section":"Config","content":"Standard Config# aws# An aws section configures AWS settings: region, profile, etc.\nIndex# An index map sets ddb.primary_key and ddb.indexes. Every index must specify a partition_key.\npartition_key# Metadata Value Description Partition key code and attribute name, like \u0026ldquo;S,Title\u0026rdquo; (required) Value Type string Valid Values Coded attribute sort_key# Metadata Value Description Sort key code and attribute name, like \u0026ldquo;N,Year\u0026rdquo; (optional) Value Type string Valid Values Coded attribute include# Metadata Value Description List of attribute included (projected) in the index Value Type List of strings Valid Values Top-level attribute names metrics# metrics.endpoint# Metadata Value Description OpenTelemetry URL Value Type string Valid Values URL Default Value metrics.freq# Metadata Value Description Frequency at which metrics are reported Value Type string Valid Values Go time duration Default \u0026ldquo;5s\u0026rdquo; metrics.percentiles# Metadata Value Description Percentiles to report Required No Value Type List of floats Valid Values Between 1 and 100 Default Value [95, 99] mysql# mysql: hostname: \u0026#34;\u0026#34; socket: \u0026#34;\u0026#34; username: \u0026#34;\u0026#34; password: \u0026#34;\u0026#34; password_file: \u0026#34;\u0026#34; db: \u0026#34;\u0026#34; table_name: \u0026#34;\u0026#34; tls: {} aws: {}hostname, socket, username, password, and db are not documented because they are standard settings that work the same in mlrd.\nmysql.password_file# Metadata Value Description File name containing password, takes precedence over mysql.password Value Type string Valid Values File name, relative or absolute path Default Value mysql.table_name# Metadata Value Description MySQL table name. Can be different than ddb.table_name, but clients always access the table by ddb.table_name, never by mysql.table_name. Value Type string Valid Values MySQL table name Default Value ddb.table_name tls# "},{"id":9,"href":"/docs/v0-beta/config/sys/","title":"Sys","section":"Config","content":"sys Config# The sys config is an optional YAML file that defines system settings and certain table config default values. The sys config is set on startup with the -sys flag.\napi# The api section configures the mlrd API.\napi.addr# Metadata Value Description Bind address and port Value Type string Valid Values Standard \u0026ldquo;addr:port\u0026rdquo; notation Default Value \u0026ldquo;:7999\u0026rdquo; api.control_plane# TODO\nlog# The log section configures mlrd logging. There are seven subsystems for logging:\ninit: Startup, config validation api : HTTP server, endpoint handlers, shutdown table: Migration, backfill, table-specific events after startup ddb: AWS and DynamoDB client sql: SQL store connection, queries expr: Expression parsing metrics: Metric recording and reporting Subsystem log levels are set by this precedent:\nAll subsystems set to debug if -debug is set All subsystems set to log.level if set init subsytem set to info, all others set to warn For example, to increase only sql subsys logging to debug:\nlog: subsys: sql: debuglog.text# Metadata Value Description Output text logs instead of JSON Value Type bool Valid Values true or false Default Value false log.level# Metadata Value Description Log level for all subsys Value Type string Valid Values error, warn, info, debug, \u0026quot;\u0026quot; (no value) Default Value log.subsys# Metadata Value Description Log level for each subsystem Value Type map Valid Values keys: subsys name\nvalues: log level Default Value init: info\napi: warn\ntable: warn\nddb: warn\nsql: warn\nexpr: warn\nmetrics: warn disable# Metadata Value Description Global list of feature flags to disable for all tables Value Type list of strings Valid Values Feature flags Default Value metrics# Metadata Value Description Bind address and port Value Type string Valid Values Standard \u0026ldquo;addr:port\u0026rdquo; notation Default Value \u0026ldquo;:7999\u0026rdquo; "},{"id":10,"href":"/docs/v0-beta/config/table/","title":"Table","section":"Config","content":"Table Config# A table config is a YAML file that defines one table in the SQL store and DDB. Every table requires a table config. Each table config is complete and independent; there are no shared table config settings or values.\nWhen mlrd starts, it reads a list of table config files, like:\n$ mlrd foo.yaml bar.yamlFiles foo.yaml and bar.yaml specify two tables.\nTable names are set by ddb.table_name and must be unique. If a client requests a table that does not exist, mlrd returns HTTP 400 and an \u0026ldquo;unknown table\u0026rdquo; error message.\nTwo settings are always required: ddb.table_name and ddb.primary_key.\nAddition setting required based on table mode:\nTable Mode Required Settings sql ddb and mysql migration ddb and mysql and migrationStep transparent ddb ddb# The ddb section defines the table for both DynamoDB and the SQL store. Settings ddb.table_name and ddb.primary_key are always required.\nddb.indexes# Metadata Value Description Secondary index definitions Value Type map Valid Values keys: index name\nvalues: Standard index Default Value ddb.primary_key# Metadata Value Description Primary key definition (required) Value Type map Valid Values Standard index Default Value ddb.table_name# Metadata Value Description Table name (required) Value Type string Valid Values DynamoDB table name Default Value disable# Metadata Value Description List of feature flags to disable in addition to sys.disable. Feature flags disabled at the system level cannot be re-enabled at the table level. Value Type list of strings Valid Values Feature flags Default Value migration_step# Metadata Value Description Migration step name Required Only when migrating a table Value Type string Valid Values Migration step name Default Value mysql# mysql standard config\n"},{"id":11,"href":"/docs/v0-beta/metrics/","title":"Metrics","section":"v0-beta","content":"Metrics# mlrd reports OpenTelemetry gauges and counters. Most metrics are table counters with a table_name tag. Metrics without a table_name tag are system metrics.\nCounters# Counters are cumulative. For example, with a 5 second reporting frequency, a counter at 50 that increases by 20 every interval reports:\nInterval Counter Value [0, 5) 50 [5, 10) 70 [10, 15) 90 Cumulative counters are standard for Prometheus and Grafana, but other platforms (like Datadog) expect counters to be deltas.\nGauges# Gauges are reported as G_max and G_pN where:\nG is the gauge name, like response_time _max is the maximum value observed during the report interval _pN is one of the configured percentiles, like G_p99 for the 99th percentile When graphing max and percentiles, use a max rollup. (Do not average percentiles.)\nThe default percentiles are 50th (median) and 99th.\nThere is no average value because averages are a common metric anti-pattern.\nTime values are milliseconds and whole number (no fractional time values).\nMetric Naming Schema and Prefixes# Metric names are lowercase and snake case, like req_table.\nPrefix Meaning backfill_ Backfill (migration) ddb_ Call to AWS DDB err_ Error legacy_ Legacy parameters projection_ Projection expression readfill_ Readfill (migration) req_ Client request sql_ Call to SQL store (MySQL) t_ Time (milliseconds) All metric names are globally unique.\nTo keep metric names short, counter metrics are logically grouped into hierarchies. (Gauge metrics are grouped but not hierarchical.) As a result, the prefix is often dropped for lower (more specific) metrics.\nCounter Groups# Counter increments cascade down a hierarchy. A top-level metric is called a \u0026ldquo;rollup metric\u0026rdquo; because it adds (rolls) up lower values. For example:\nreq req_table req_write req_delete req_delete_cond req_delete_cond_failEvery request increments the req. If the request is a table request, it also increments req_table. If the table request is a write, it also increments req_write. If the write is also a delete (DeleteItem), it also increments req_delete. If the delete is conditional (ConditionExpression is set), it also increments req_delete_cond. And if the condition fails (does not pass), it also increments req_delete_cond_fail.\nTherefore: req \u0026gt; req_table \u0026gt; req_write \u0026gt; req_delete \u0026gt; req_delete_cond \u0026gt; req_delete_cond_fail. This makes it easy to graph, for example, the percentage of conditional deletes that fail: req_delete_cond_fail / req_delete_cond * 100.\nThere are 5 counter groups:\nClient Request (req_) DDB Request (ddb_) SQL store access (sql_) Migration (backfill_ and readfill_) Error (err_, fault, and panic) Most metrics are table counters, but * denotes system counters (no table_name tag).\nClient Request# Client request metrics show what clients are requesting. Normally, these metrics would also reflect server activity (for example, a client requests a read and the server does a read), but mlrd is a three-part system: API (mlrd), DDB, and a SQL store. Especially during a migration, a single client request can result in multiple server-side requests. Therefore, client request metrics are only a starting point for monitoring mlrd.\nreq* req_fwd* req_ctrl* req_table req_read req_get_batch req_get_trx req_get_one req_get_one_nop req_query req_query_index req_query_page req_scan req_scan_index req_scan_page check_write_cond req_write req_write_batch req_write_trx req_put req_put_cond req_put_cond_fail req_delete req_delete_cond req_delete_cond_fail req_update req_update_cond req_update_cond_fail legacy_expr_cond legacy_expr_filter legacy_expr_key legacy_expr_projection legacy_expr_update projection_no_attribute projection_not_a_list projection_list_index_oobDDB Request# DDB request metrics track calls from mlrd to AWS on behalf of clients. Although AWS provides metrics for DDB, mlrd metrics are higher resolution (5s by default). These metrics are also important during a migration to monitor the additional DDB request necessary to backfill items into SQL.\nddb_get_item ddb_put_item ddb_update_item ddb_delete_item ddb_batch_get_item ddb_batch_write_item ddb_query ddb_scan ddb_ctrl ddb_fwdSQL Store Access# SQL store access metrics show data access to MySQL. Although MySQL can (and should) be monitored separately, these metrics show access from the client side (mlrd is a client to MySQL). During a migration it\u0026rsquo;s important to monitor SQL read/write access with respect to DDB read/write access: the former (SQL) eventually takes over for the latter (DDB).\nsql_read sql_read_row sql_read_row_nop sql_read_set sql_scan sql_scan_index sql_scan_filter sql_scan_key_cond sql_scan_page sql_scan_count sql_scan_nop sql_write sql_read_snapshot sql_read_cond sql_insert sql_update sql_upsert sql_delete sql_delete_nopMigration# Migration metrics show backfill and readfill work.\nbackfill_write backfill_rows_written readfill_read readfill_rows_missing readfill_write readfill_rows_writtenError# Error metrics count every error mlrd can return. The top-most (system) counter, err, should be monitored to establish a baseline (normal) error rate. (Production systems usually have a non-zero error rate due unavoidable events like client discounts, network blips, and so on.)\npanic is in the error group but does not have an err_ prefix because it\u0026rsquo;s more of a bug (in mlrd) than an error. fault is a serious, nonrecoverable (but caught) failure that means the SQL store data is invalid.\nerr* err_http_read* err_http_write* err_decode_input* err_encode_output* err_unknown_table* err_table err_ddb err_sql err_trx err_backfill err_readfill err_encode_row err_decode_row err_metrics err_client err_unknown_index err_no_key_value err_expr_cond err_expr_filer err_expr_key err_expr_projection err_expr_update panic* fault*Gauge Groups# Gauges are also grouped but not hierarchical. There are 2 gauge groups:\nResponse Time System Response Time# Response time gauges report how long requests and data access took (in milliseconds). Since these are server-side measurements (inside mlrd), clients will observer higher response times due to network latency that mlrd cannot measure.\nt_req* t_req_table_read t_req_table_write t_req_batch_read* t_req_batch_write* t_sql_read_row t_sql_read_set t_sql_scan t_sql_insert t_sql_update t_sql_delete t_ddb_read t_ddb_writeSystem# Currently, there is only 1 system gauge group that measures system load (active requests).\nload*Batch Sizes# size_get_batch_items size_write_batch_itemsMetric Reference# Type: C=counter; G=gauge System ✓: system metric; no table_name tag\nName Type System Description backfill_write C Number of SQL writes for backfilling data. backfill_rows_written C Number of rows backfilled into SQL on write. check_write_cond C Number of reads for a ConditionCheck in a TransactWriteItems request. ddb_batch_get_item C Number of BatchGetItem calls to DDB. ddb_batch_write_item C Number of BatchWriteItem calls to DDB. ddb_ctrl C Number of control plan calls to DDB in addition to req_ctrl. ddb_delete_item C Number of DeleteItem calls to DDB. ddb_fwd C Number of requests forwarded to DDB (transparent proxy). ddb_get_item C Number of GetItem calls to DDB. ddb_put_item C Number of PutItem calls to DDB. ddb_query C Number of Query calls to DDB. ddb_scan C Number of Scan calls to DDB. ddb_update_item C Number of UpdateItem calls to DDB. err C ✓ Number of all errors. err_backfill C Number of errors backfilling SQL on write. err_client C Number of client errors: invalid syntax, missing values, etc. Monitor this rollup metric per-table. err_ddb C Number of errors returned by AWS/DDB, returned verbatim to client. err_decode_input C ✓ Number of errors decoding (unmarshalling) JSON input. err_decode_row C Number of errors decoding (unmarshalling) JSON from SQL. err_encode_output C ✓ Number of errors encoding (marshalling) JSON output. err_encode_row C Number of errors encoding (marshaling) JSON for SQL. err_expr_cond C Number of conditional expression syntax errors. err_expr_filer C Number of filter expression syntax errors. err_expr_key C Number of key condition syntax errors. err_expr_projection C Number of projection expression syntax errors. err_expr_update C Number of update expression syntax errors. err_http_read C ✓ Number of HTTP read errors. err_http_write C ✓ Number of HTTP write errors. err_metrics C Number of errors sending (reporting) metrics. err_no_key_value C Number of requests that do not specify one or more required key value. err_readfill C Number of errors backfilling SQL on read. Reported in log as warnings. err_sql C Number of errors returned by SQL. err_table C ✓ Number of table access errors. Monitor this rollup metric for each table. err_trx C Number of errors on SQL BEGIN, COMMIT, or ROLLBACK. err_unknown_index C Number of requests that specify an unknown secondary index. err_unknown_table C ✓ Number of requests that specify an unknown table. fault C ✓ Number of serious runtime failures. This value should always be zero. legacy_expr_cond C Number of requests that use legacy parameters instead of ConditionExpression. legacy_expr_filter C Number of requests that use legacy parameters instead of FilterExpression. legacy_expr_key C Number of Query requests that use legacy parameters instead of KeyConditionExpression. legacy_expr_projection C Number of read requests that use legacy parameters instead of ProjectionExpression. legacy_expr_update C Number of update requests that use legacy parameters instead of UpdateExpression. load G ✓ Number of active requests. Incremented by 1 when a request begins and decremented by 1 when the request ends. panic C ✓ Number of runtime panics. projection_no_attribute C Number of attributes projected but not present in each item. projection_not_a_list C Number of attributes projected as a list type but not a list in each item. projection_list_index_oob C Number of attributes projected as a list but out of bounds for the list in each item. readfill_read C Number of SQL reads for read-filling data. readfill_rows_missing C Number of rows missing in SQL (for missfill). readfill_write C Number of SQL writes for read-filling data. readfill_rows_written C Number of rows backfilled into SQL on read. req C ✓ Number of all requests. Includes control plane requests like ListTables. req_ctrl C ✓ Number of control plane requests handled by mlrd. req_delete C Number of delete requests. req_delete_cond C Number of delete requests with a conditional expression. req_delete_cond_fail C Number of conditional delete requests that fail the condition check. req_fwd C ✓ Number of requests forwarded to DDB, usually control plane requests. req_get_batch C Number of batch read item requests (BatchReadItem). req_get_one C Number of single-item get requests (GetItem). req_get_one_nop C Number of single-item get requests that did not match an item. req_put C Number of put (insert) requests. req_put_cond C Number of put requests with a conditional expression. req_put_cond_fail C Number of conditional insert requests that fail the condition check. req_query C Number of query requests (Query). req_query_index C Number of query requests that specify a secondary index (not the primary key). req_query_page C Number of query requests that paginate by specifying starting key values (ExclusiveStartKey). req_read C Number of table data read requests. req_scan C Number of scan requests (Scan). req_scan_index C Number of scan requests that specify a secondary index (not the primary key). req_scan_page C Number of scan requests that paginate by specifying starting key values (ExclusiveStartKey). req_table C Number of table data access requests (all table read and write requests). Use this metric to monitor table throughput as requests per second (RPS). req_update C Number of update requests. req_update_cond C Number of update requests with a conditional expression. req_update_cond_fail C Number of conditional update requests that fail the condition check. req_write C Number of table data write requests. req_write_batch C Number of batch write requests (BatchWriteItem). size_get_batch_items G Number of items requested in BatchGetItem. size_write_batch_items G Number of writes requested in BatchWriteItem. sql_delete C Number of SQL DELETE executed (not rows deleted). sql_delete_nop C Number of SQL DELETE that did not match a row. sql_insert C Number of SQL INSERT executed (not rows inserted). sql_read C Number of SQL reads: row, set, and scan. sql_read_cond C Number of SQL single-row reads for checking write condition. sql_read_row C Number of SQL single-row reads (point lookups). sql_read_row_nop C Number of SQL single-row reads that did not match a row. sql_read_set C Number of SQL multi-row reads. sql_read_snapshot C Number of SQL single-row reads for before/after/modified write values. sql_scan C Number of SQL multi-row reads by scanning the primary key or an index (not rows read). sql_scan_index C Number of SQL multi-row reads by scanning a secondary index. sql_scan_filter C Number of SQL multi-row reads with a filter condition. sql_scan_key_cond C Number of SQL multi-row reads with a key condition. sql_scan_page C Number of SQL multi-row reads with an index offset to fetch next page of result set. sql_scan_count C Number of SQL multi-row reads that SELECT COUNT(*) instead of returning rows. sql_scan_nop C Number of SQL multi-row reads that match zero rows (empty result set) because conditions don\u0026rsquo;t match or no more pages. sql_update C Number of SQL UPDATE executed (not rows updated). sql_upsert C Number of SQL INSERT due to no row on UPDATE. sql_write C Number of SQL writes: INSERT, UPDATE, DELETE. t_req G ✓ Response time of all requests. t_req_batch_read G ✓ Response time of batch read requests. t_req_batch_write G ✓ Response time of batch write requests. t_req_table_read G Response time of table read requests. t_req_table_write G Response time of table write requests. t_sql_read_row G Response time of SQL single-row reads. t_sql_read_set G Response time of SQL multi-row reads. t_sql_scan G Response time of SQL scans. t_sql_delete G Response time of SQL deletes. t_sql_insert G Response time of SQL inserts. t_sql_update G Response time of SQL updates. t_ddb_read G Response time of DDB read requests. t_ddb_write G Response time of DDB write requests. "},{"id":12,"href":"/docs/v0-beta/migration/overview/","title":"Overview","section":"Migration","content":"Migration Migrating a table from DynamoDB to mlrd is standard four-part process:\nPart Goal mlrd Migration Steps Saftey Interlock 1 Ensure mlrd compatibility 1 2 Backfill and dual-write data 2, 3, 4 Backfill Lock 3 Verify SQL store 5, 6 Write barrier 1 4 Cut over to SQL store 7 Write barrier 2 These are the steps:\n#| Step | Read1 | Read2 || Write1 | Write2 | |-------------|----------|----------||--------|--------| 1| ddb | ddb | || ddb | | |- - - - - - - - - - - - - - - - - - - - - - - - - - - |Backfill Lock 2| backfill | ddb | || ddb | mysql | 3| readfill | ddb -\u0026gt; W2| || ddb | mysql | 4| missfill | ddb |mysql-\u0026gt; W2|| ddb | mysql | |------------------------------------------------------|Write Barrier 1 5| trx-write | mysql | || trx\u0026lt;mysql, ddb\u0026gt; | 6| pre-sql | mysql | || mysql | ddb | |======================================================|Write Barrier 2 7| sql | mysql | || mysql | |The ASCII table above shows read and write order for each step. The -\u0026gt; W2 in steps 3 and 4 means \u0026ldquo;then Write2: mysql\u0026rdquo;. The trx\u0026lt;\u0026gt; in step 5 means \u0026ldquo;in a SQL transaction\u0026rdquo;.\nStep 1: ddb# Step 1 is easy: connect the app to mlrd instead of DynamoDB, and mlrd reads and writes to DynamoDB on behalf of the app.\nThis is done to ensure mlrd can completely and correctly handle the app workload. As noted in Table Modes, mlrd is not transparent in the ddb migration step; any mlrd errors are returned to the client.\nIf the app and mlrd work correctly in ddb for a few days or weeks, then it\u0026rsquo;s time to backfilling data into the MySQL—the SQL store.\nSteps 2–4: Backfill# Steps 2–4 introduce dual writes to help backfill data from DynamoDB to MySQL.\nOnly one mlrd instance back run in a backfill step. This is enforced by requiring the backfill lock.\nThere are critically important reasons for backfill lock safety; see Part 2: Backfill for details.\nmlrd dual writing to DynamoDB and then MySQL is only half the backfill. The other half is dumping data from DynamoDB and loading into MySQL, which is done separately.\nSteps 5 and 6: Verify SQL# Steps 5 and 6 change the source of truth from DynamoDB to MySQL with the ability to roll back.\nMySQL becomes Write1 and DynamoDB becomes Write2.\nThese steps verify the SQL store in two ways:\nData: Reads are served from MySQL, and configurable amount of shadow reads are made from DynamoDB to verify that MySQL and DynamoDB return the same data Load: Full read and write workload reveals if the SQL store performs up to expectations For a mlrd instance to run in either step, write barrier 1 must exist; see Safety for details.\nRolling forward from backfill steps to these steps requires that all writes be stopped; see Part 3: Verify SQL for details.\nRolling back to backfill steps (to make DynamoDB the Write1 source of truth again) is possible, too, as long as no faults occur during these steps.\nStep 7: Cut Over# Step 7 stops dual writing to DynamoDB.\nAs such, there\u0026rsquo;s no return to DynamoDB after step 7 because the moment a single mlrd instances runs in step 7 and writes to MySQL, that write is not present in DynamoDB, which means DynamoDB data is invalid.\n"},{"id":13,"href":"/docs/v0-beta/migration/safety/","title":"Safety","section":"Migration","content":"Safety Interlocks# Migration safety interlocks prevent mlrd from writing data unless certain conditions are true. Interlocks exist the _migration table:\nCREATE TABLE _migration ( `table_name` varchar(128) NOT NULL, `interlock` varchar(128) NOT NULL, `value` varchar(128) DEFAULT NULL, PRIMARY KEY (table_name, interlock) );These are the interlocks:\nInterlock Value backfill-lock mlrd instance ID that has acquired the backfill lock write-barrier 1 or 2 To create an interlock, insert the row:\nINSERT IGNORE INTO mlrd._migration (tableName, interlock) VALUES (\u0026#39;Foo\u0026#39;, \u0026#39;write-barrier-1\u0026#39;);Why Writes Must Stop# "},{"id":14,"href":"/docs/v0-beta/supported-requests/","title":"Supported Requests","section":"v0-beta","content":"Supported Requests# Classic API: fully supported Control Plane: when applicable TTL: not supported PartiQL: not supported Streams: not supported Request Supported? Notes BatchExecuteStatement No PartiQL not supported BatchGetItem ✅ BatchWriteItem ✅ CreateBackup No CreateGlobalTable No CreateTable ✅ DeleteBackup No DeleteItem ✅ DeleteResourcePolicy No DeleteTable ✅ DescribeBackup No DescribeContinuousBackups No DescribeContributorInsights No DescribeEndpoints No DescribeExport No DescribeGlobalTable No DescribeGlobalTableSettings No DescribeImport No DescribeKinesisStreamingDestination No DescribeLimits No DescribeTable ✅ DescribeTableReplicaAutoScaling No DescribeTimeToLive No DisableKinesisStreamingDestination No EnableKinesisStreamingDestination No ExecuteStatement No PartiQL not supported ExecuteTransaction No PartiQL not supported ExportTableToPointInTime No GetItem ✅ GetResourcePolicy No ImportTable No ListBackups No ListContributorInsights No ListExports No ListGlobalTables No ListImports No ListTables ✅ ListTagsOfResource No PutItem ✅ PutResourcePolicy No Query ✅ RestoreTableFromBackup No RestoreTableToPointInTime No Scan ✅ TagResource No TransactGetItems In progress TransactWriteItems In progress UntagResource No UpdateContinuousBackups No UpdateContributorInsights No UpdateGlobalTable No UpdateGlobalTableSettings No UpdateItem ✅ UpdateKinesisStreamingDestination No UpdateTable Planned UpdateTableReplicaAutoScaling No UpdateTimeToLive No "},{"id":15,"href":"/docs/v0-beta/config/mysql/","title":"MySQL","section":"Config","content":"MySQL# mlrd is developed on MySQL Community Edition v8.4 but also supports:\nMySQL Version Tested Supported Recommended ≥ v8.0.32 ✅ ✅ v8.4 LTS ✅ ✅ ✅ v9.x User# Presuming:\nUser: mlrd Password: ... Database: mlrd Use caching_sha2_password and grant full access only to the database that mlrd uses:\nCREATE USER IF NOT EXISTS mlrd IDENTIFIED WITH caching_sha2_password BY \u0026#39;...\u0026#39;; GRANT ALL ON mlrd.* TO mlrd@\u0026#39;%\u0026#39;;"},{"id":16,"href":"/docs/v0-beta/migration/part-1/","title":"Part 1: Compatiblity","section":"Migration","content":"Part 1: Ensure mlrd Compatibility Deploy a canary in ddb mode, verify compatibility Deploy all in ddb mode; no change to anything "},{"id":17,"href":"/docs/v0-beta/migration/part-2/","title":"Part 2: Backfill","section":"Migration","content":"Part 2: Backfill and Dual-write Data Create empty backfill lock: INSERT INTO mlrd._migration (tableName, flag) VALUES ('...', 'backfill') Start one mlrd in a backfill mode Dump DynamoDB table and load into MySQL but do not overwrite existing items because anything already present in MySQL is from a write DynamoDB is written to first and remains authoritative (the source of truth) MySQL is written to second (as shown in the Write2 column above) Backfill writes are authoritative Items in MySQL are newest: either from a mirror write or a backfill MySQL write error during backfill: OK, just be sure to log and retry/re-backfill the item(s)\n"},{"id":18,"href":"/docs/v0-beta/migration/part-3/","title":"Part 3: Verify SQL","section":"Migration","content":"Part 3: Verify SQL Store When backfill is done, dynamically change mode to trx-write to read from MySQL, write to MySQL and DDB\u0026ndash;this will disable writes When all nodes in trx-write step, dynamically enable writes Set migrationStep: trx-write in config and roll out "},{"id":19,"href":"/docs/v0-beta/migration/part-4/","title":"Part 4: Cut Over","section":"Migration","content":"Part 4: Cut Over to SQL Store Roll out deploy with migrationStep: sql Verify everything still works Remove migrationStep and roll out again to put table in SQL mode Option to make canary write PK to file. on rollback, read PK from MySQL, write to DDB\n"}]